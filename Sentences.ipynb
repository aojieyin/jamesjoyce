{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded text file\n",
    "file_path = 'Portrait.txt'\n",
    "file_path1 = 'Dubliners.txt'\n",
    "file_path3 = 'Ulysses.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path3, 'r',encoding='utf-8') as file:\n",
    "    text_content = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "# Removing extra whitespace and newlines\n",
    "cleaned_text = re.sub(r'\\s+', ' ', text_content.strip())\n",
    "\n",
    "cleaned_text = re.sub(r'[“”]', '\"', cleaned_text)  # Replace double curly quotes\n",
    "\n",
    "cleaned_text = re.sub(r'[‘’]', \"'\", cleaned_text)  # Replace single curly quotes\n",
    "\n",
    "cleaned_text = re.sub(r'\\.{2,}', '.', cleaned_text) # Replace the ellipsis\n",
    "\n",
    "# Remove non-essential characters \n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\-\\—\\\"\\s]', '',  cleaned_text)\n",
    "\n",
    "cleaned_text = re.sub(r'—', ' ', cleaned_text)\n",
    "\n",
    "# Lowercase the text for case-insensitive analysis\n",
    "cleaned_text = cleaned_text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Length and Its Average, Median, Min, Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize, pos_tag, FreqDist\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "# Calculate sentence lengths\n",
    "sentence_lengths = [\n",
    "    (sentence, len([word for word in word_tokenize(sentence) if word.isalpha() or '-' in word]))\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "\n",
    "lengths = [length for _, length in sentence_lengths]\n",
    "\n",
    "total_sentences = len(sentences)\n",
    "\n",
    "mean_sentence_length = sum(lengths) / len(lengths) if lengths else 0\n",
    "\n",
    "median_sentence_length = statistics.median(lengths)\n",
    "\n",
    "min_sentence_length = min(lengths)\n",
    "\n",
    "max_sentence_length = max(lengths)\n",
    "\n",
    "sorted_sentences = sorted(sentence_lengths, key=lambda x: x[1], reverse=True)\n",
    "top_10_sentences = sorted_sentences[:10]\n",
    "\n",
    "top_10_df = pd.DataFrame(top_10_sentences, columns=[\"Sentence\", \"Word Count\"])\n",
    "\n",
    "print(f\"Total: {total_sentences}\")\n",
    "print(f\"Mean: {mean_sentence_length}\")\n",
    "print(f\"Median: {median_sentence_length}\")\n",
    "print(f\"Min: {min_sentence_length}\")\n",
    "print(f\"Max: {max_sentence_length}\")\n",
    "\n",
    "top_10_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "# Generate N-grams\n",
    "trigrams_list = list(ngrams(tokens, 6))\n",
    "# Calculate frequencies\n",
    "freq_dict = Counter(trigrams_list)\n",
    "filtered_freq_dict = {ngram: freq for ngram, freq in freq_dict.items() if freq >= 5}\n",
    "\n",
    "# Calculate types (unique N-grams) and tokens (total N-grams)\n",
    "total_trigram_tokens = sum(filtered_freq_dict.values())\n",
    "unique_trigram_types = len(filtered_freq_dict)\n",
    "\n",
    "print(\"Total Trigram Tokens:\", total_trigram_tokens)\n",
    "print(\"Unique Trigram Types:\", unique_trigram_types)\n",
    "\n",
    "#import csv       \n",
    "#output_file_path = \"5grams_output.csv\"\n",
    "#with open(output_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    #writer = csv.writer(csvfile)\n",
    "    #writer.writerow([\"N-gram\", \"Frequency\"])  # Write header\n",
    "    #for ngram, freq in freq_dict.items():\n",
    "        #writer.writerow([' '.join(ngram), freq]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "\n",
    "for sentence in sentences:\n",
    "        # Tokenize words in the sentence\n",
    "    words = [word for word in word_tokenize(sentence) if word.isalpha() or '-' in word]\n",
    "        # Tag words with parts of speech\n",
    "    pos_tags = pos_tag(words)\n",
    "        # Extract the POS sequence\n",
    "    pattern = '-'.join(tag for word, tag in pos_tags)\n",
    "    patterns.append(pattern)\n",
    "\n",
    "#output_file = 'patterns_output1.txt'\n",
    "#with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    #for pattern, sentence in patterns:\n",
    "        #file.write(f\"Pattern: {pattern}\\nSentence: {sentence}\\n\\n\")\n",
    "        \n",
    "    # Count frequencies of patterns\n",
    "pattern_counts = Counter(patterns)\n",
    "print(len(pattern_counts))\n",
    "\n",
    "for pattern, count in pattern_counts.most_common(30):\n",
    "    print(f\"{pattern}: {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
