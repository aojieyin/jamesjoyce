{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded text file\n",
    "file_path = 'Ulysses.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r',encoding='utf-8') as file:\n",
    "    text_content = file.read()\n",
    "\n",
    "# Text preprocessing: remove extra spaces and standardize format\n",
    "import re\n",
    "\n",
    "# Removing extra whitespace and newlines\n",
    "cleaned_text = re.sub(r'\\s+', ' ', text_content.strip())\n",
    "\n",
    "cleaned_text = re.sub(r'[\\“\\”]', '\"', cleaned_text)  # Replace double curly quotes\n",
    "\n",
    "cleaned_text = re.sub(r'[\\‘\\’]', \"'\", cleaned_text)  # Replace single curly quotes\n",
    "\n",
    "cleaned_text = re.sub(r'\\.{2,}', '.', cleaned_text) # Replace the ellipsis\n",
    "\n",
    "# Remove non-essential characters (if necessary, like extra formatting artifacts)\n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\-\\—\\\"\\s]', '',  cleaned_text)\n",
    "\n",
    "cleaned_text = re.sub(r'—', ' ', cleaned_text)\n",
    "\n",
    "# Lowercase the text for case-insensitive analysis\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "#output_file_path = 'Cleaned.txt'\n",
    "\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    #output_file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Tokens, Types, Hapax Legomena, TTR, STTR, Lexical Density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, FreqDist\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "tokens = [token for token in tokens if token.isalpha() or '-' in token ]\n",
    "    \n",
    "# Calculate the word count\n",
    "word_count = len(tokens)\n",
    "\n",
    "# Calculate unique words (types)\n",
    "unique_words = set(tokens)\n",
    "unique_word_count = len(unique_words)\n",
    "\n",
    "# Calculate hapax legomena\n",
    "freq_dist = FreqDist(tokens)\n",
    "hapax_legomena = [word for word, count in freq_dist.items() if count == 1]\n",
    "\n",
    "# Calculate type-token ratio (TTR)\n",
    "ttr = unique_word_count / word_count if word_count > 0 else 0\n",
    "\n",
    "# Calculate STTR\n",
    "chunk_size = 1000\n",
    "\n",
    "chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "ttrs = []\n",
    "for chunk in chunks:\n",
    "    unique_tokens = set(chunk)\n",
    "    ttr_each_chunk = len(unique_tokens) / len(chunk) if len(chunk) > 0 else 0\n",
    "    ttrs.append(ttr_each_chunk)\n",
    "\n",
    "sttr = sum(ttrs) / len(ttrs) if ttrs else 0\n",
    "\n",
    "# Calculate lexical density\n",
    "pos_tags = pos_tag(tokens)\n",
    "content_words = [word for word, tag in pos_tags if tag.startswith(('NN', 'VB', 'JJ', 'RB'))]\n",
    "lexical_density = (len(content_words) / word_count) * 100 if word_count > 0 else 0\n",
    "\n",
    "# Output results\n",
    "print(f\"Total Word Count: {word_count}\")\n",
    "print(f\"Unique Word Count: {unique_word_count}\")\n",
    "print(f\"Hapax Legomena Count: {len(hapax_legomena)}\")\n",
    "print(f\"Type-Token Ratio (TTR): {ttr:.2f}\")\n",
    "print(f\"Standardized Type-Token Ratio (STTR):{sttr:.2f}\")\n",
    "print(f\"Lexical Density: {lexical_density:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "stop_words = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stop_words = set(stop_words.decode().splitlines()) \n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Count token frequencies\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "# Get the top 100 most frequent tokens\n",
    "top_100 = freq_dist.most_common(100)\n",
    "\n",
    "# Display the results\n",
    "for token, count in top_100:\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_100))\n",
    "    \n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(f\"Top 100 Words in {file_path}\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "# Count total tokens with punctuation marks\n",
    "tokens_with_punct_count = len(word_tokenize(cleaned_text))\n",
    "\n",
    "print(tokens_with_punct_count)\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Count total punctuation marks\n",
    "punctuation_count = sum(1 for char in cleaned_text if char in punctuations)\n",
    "\n",
    "print(punctuation_count)\n",
    "\n",
    "# \n",
    "exclamation_count = cleaned_text.count('!')\n",
    "\n",
    "print(exclamation_count)\n",
    "\n",
    "percentage = punctuation_count / tokens_with_punct_count\n",
    "\n",
    "print(percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
